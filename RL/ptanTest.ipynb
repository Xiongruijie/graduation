{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import parl\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import enviroment\n",
    "from paddle.distribution import Normal\n",
    "from parl.utils import logger\n",
    "import argparse\n",
    "from itertools import count\n",
    "from parl.utils import logger, summary, ReplayMemory\n",
    "from collections import deque\n",
    "import random\n",
    "from visualdl import LogWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Actor(parl.Model):\n",
    "    def __init__(self, action_dim=3, is_train=True):\n",
    "        self.action_dim = action_dim\n",
    "        self.is_train = is_train\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2D(1, 96, 11, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(3, 3),\n",
    "            nn.Conv2D(96, 256, 3, 1, 1),\n",
    "            nn.MaxPool2D(3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(256, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(3, 2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(20800, 4096),#64 * 9 * 18\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.action_dim),\n",
    "        )\n",
    "        self.noisy = Normal(0, 0.2)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        feature = self.conv(obs)\n",
    "        feature = feature.reshape([obs.shape[0], -1])\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "\n",
    "    def select_action(self, epsilon, state):\n",
    "        state = paddle.to_tensor(state, dtype=\"float32\")\n",
    "        with paddle.no_grad():\n",
    "            action = self.forward(state)\n",
    "            action = action + self.is_train * epsilon * self.noisy.sample(action.shape)\n",
    "        return action.numpy()\n",
    "        # return action.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "class Critic(parl.Model):\n",
    "    def __init__(self, action_dim=3, ):\n",
    "        super(Critic, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2D(1, 96, 11, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(3, 3),\n",
    "            nn.Conv2D(96, 256, 3, 1, 1),\n",
    "            nn.MaxPool2D(3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(256, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(3, 2),\n",
    "        )\n",
    "        # 因为中间要加动作判定所以需要fc分层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 9 * 18, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(2048 + 3, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        feature = self.conv(obs)\n",
    "        feature = feature.reshape([obs.shape[0], -1])\n",
    "        output = self.fc(feature)\n",
    "\n",
    "        output = paddle.concat((output, action), axis=1)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# class MujocoAgent(parl.Agent):\n",
    "#     def __init__(self, algorithm, act_dim, expl_noise=0.1):\n",
    "#         assert isinstance(act_dim, int)\n",
    "#         super(MujocoAgent, self).__init__(algorithm)\n",
    "#         self.act_dim = act_dim\n",
    "#         self.expl_noise = expl_noise\n",
    "#         self.alg.sync_target(decay=0)\n",
    "#\n",
    "#     def sample(self, obs):\n",
    "#         action_numpy = self.predict(obs)\n",
    "#         action_noise = np.random.normal(0, self.expl_noise, size=self.act_dim)\n",
    "#         action = (action_numpy + action_noise).clip(-1, 1)\n",
    "#         return action\n",
    "#\n",
    "#     def predict(self, obs):\n",
    "#         obs = paddle.to_tensor(obs.reshape(1, -1), dtype='float32')\n",
    "#         action = self.alg.predict(obs)\n",
    "#         action_numpy = action.cpu().numpy()[0]\n",
    "#         return action_numpy\n",
    "#\n",
    "#     def learn(self, obs, action, reward, next_obs, terminal):\n",
    "#         terminal = np.expand_dims(terminal, -1)\n",
    "#         reward = np.expand_dims(reward, -1)\n",
    "#\n",
    "#         obs = paddle.to_tensor(obs, dtype='float32')\n",
    "#         action = paddle.to_tensor(action, dtype='float32')\n",
    "#         reward = paddle.to_tensor(reward, dtype='float32')\n",
    "#         next_obs = paddle.to_tensor(next_obs, dtype='float32')\n",
    "#         terminal = paddle.to_tensor(terminal, dtype='float32')\n",
    "#         critic_loss, actor_loss = self.alg.learn(obs, action, reward, next_obs,\n",
    "#                                                  terminal)\n",
    "#         return critic_loss, actor_loss\n",
    "\n",
    "\n",
    "# class MujocoModel(parl.Model):\n",
    "#     def __init__(self, action_dim):\n",
    "#         super(MujocoModel, self).__init__()\n",
    "#         self.actor_model = Actor(action_dim)\n",
    "#         self.critic_model = Critic(action_dim)\n",
    "#\n",
    "#     def policy(self, obs):\n",
    "#         return self.actor_model(obs)\n",
    "#\n",
    "#     def value(self, obs, action):\n",
    "#         return self.critic_model(obs, action)\n",
    "#\n",
    "#     def get_actor_params(self):\n",
    "#         return self.actor_model.parameters()\n",
    "#\n",
    "#     def get_critic_params(self):\n",
    "#         return self.critic_model.parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, memory_size) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=self.memory_size)\n",
    "\n",
    "    def add(self,experience)->None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int, continuous: bool = False):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if continuous:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "        else:\n",
    "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "            return [self.buffer[i] for i in indexes]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.set_value( target_param * (1.0 - tau) + param * tau)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[10-14 22:04:03 MainThread @2137709701.py:7]\u001B[0m ------------------ DDPG ---------------------\n",
      "\u001B[32m[10-14 22:04:03 MainThread @2137709701.py:8]\u001B[0m Env: CircuitEnv, Seed: 0\n",
      "\u001B[32m[10-14 22:04:03 MainThread @2137709701.py:9]\u001B[0m ---------------------------------------------\n",
      "Tensor(shape=[1, 1, 1031, 1865], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[[[1., 1., 1., ..., 1., 1., 1.],\n",
      "          [1., 1., 1., ..., 1., 1., 1.],\n",
      "          [1., 1., 1., ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1., ..., 1., 1., 1.],\n",
      "          [1., 1., 1., ..., 1., 1., 1.],\n",
      "          [1., 1., 1., ..., 1., 1., 1.]]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "(InvalidArgument) arg_max(): argument 'X' (position 0) must be Tensor, but got numpy.ndarray (at /paddle/paddle/fluid/pybind/op_function_common.cc:737)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 106>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;66;03m# parser.add_argument(\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m#     \"--train_total_steps\",\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m#     default=5e6,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m#     default=int(5e3),\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m#     help='The step interval between two consecutive evaluations')\u001B[39;00m\n\u001B[1;32m    121\u001B[0m args \u001B[38;5;241m=\u001B[39m parser\u001B[38;5;241m.\u001B[39mparse_args(args\u001B[38;5;241m=\u001B[39m[])\n\u001B[0;32m--> 122\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m time_step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m99\u001B[39m):\n\u001B[1;32m     40\u001B[0m     action \u001B[38;5;241m=\u001B[39m actor\u001B[38;5;241m.\u001B[39mselect_action(epsilon, state)\n\u001B[0;32m---> 41\u001B[0m     next_state, reward, done \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     episode_reward \u001B[38;5;241m=\u001B[39m reward\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;66;03m# reward = (reward + 8.1) / 8.1\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/graduation/RL/enviroment.py:136\u001B[0m, in \u001B[0;36mCircuitEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m    133\u001B[0m \n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# 计算action 输入然后查找对应的解\u001B[39;00m\n\u001B[1;32m    135\u001B[0m     is_terminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_index \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m next_state\n",
      "File \u001B[0;32m~/Documents/graduation/RL/enviroment.py:76\u001B[0m, in \u001B[0;36mCircuitEnv.reward\u001B[0;34m(self, action, index)\u001B[0m\n\u001B[1;32m     74\u001B[0m choose_action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m############################\u001B[39;00m\n\u001B[0;32m---> 76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m paddle\u001B[38;5;241m.\u001B[39mequal(\u001B[43mpaddle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m, fdm):\n\u001B[1;32m     77\u001B[0m     choose_action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfdm\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m paddle\u001B[38;5;241m.\u001B[39mequal(paddle\u001B[38;5;241m.\u001B[39margmax(action), bem):\n",
      "File \u001B[0;32m~/anaconda3/envs/graduation/lib/python3.8/site-packages/paddle/tensor/search.py:181\u001B[0m, in \u001B[0;36margmax\u001B[0;34m(x, axis, keepdim, dtype, name)\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _C_ops\u001B[38;5;241m.\u001B[39mfinal_state_argmax(x, axis, keepdim, flatten, var_dtype)\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _in_legacy_dygraph():\n\u001B[0;32m--> 181\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43m_C_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marg_max\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maxis\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdtype\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkeepdims\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mkeepdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mflatten\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflatten\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m    185\u001B[0m helper \u001B[38;5;241m=\u001B[39m LayerHelper(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margmax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlocals\u001B[39m())\n",
      "\u001B[0;31mValueError\u001B[0m: (InvalidArgument) arg_max(): argument 'X' (position 0) must be Tensor, but got numpy.ndarray (at /paddle/paddle/fluid/pybind/op_function_common.cc:737)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    logger.info(\"------------------ DDPG ---------------------\")\n",
    "    logger.info('Env: {}, Seed: {}'.format(args.env, args.seed))\n",
    "    logger.info(\"---------------------------------------------\")\n",
    "    logger.set_dir('./{}_{}'.format(args.env, args.seed))\n",
    "\n",
    "    explore = 50000\n",
    "    epsilon = 1\n",
    "    gamma = 0.99\n",
    "    tau = 0.001\n",
    "    learn_steps = 0\n",
    "    writer = LogWriter('logs')\n",
    "\n",
    "\n",
    "    actor = Actor()\n",
    "    critic = Critic()\n",
    "    actor_target = Actor()\n",
    "    critic_target = Critic()\n",
    "    env = enviroment.CircuitEnv(0, 0, 99)\n",
    "\n",
    "    # 优化器\n",
    "    critic_optim = paddle.optimizer.Adam(parameters=critic.parameters(), learning_rate=3e-5)\n",
    "    actor_optim = paddle.optimizer.Adam(parameters=actor.parameters(), learning_rate=1e-5)\n",
    "    memory_replay = Memory(50000)\n",
    "    begin_train = False\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in count():\n",
    "        state = env.reset()\n",
    "        print(env.get_state())\n",
    "        episode_reward = 0\n",
    "        for time_step in range(99):\n",
    "            action = actor.select_action(epsilon, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward = reward\n",
    "            # reward = (reward + 8.1) / 8.1\n",
    "            memory_replay.add((state, next_state, action, reward))\n",
    "            if memory_replay.size() > 1280:\n",
    "                learn_steps += 1\n",
    "                if not begin_train:\n",
    "                    print('train begin!')\n",
    "                    begin_train = True\n",
    "            experiences = memory_replay.sample(batch_size, False)\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = zip(*experiences)\n",
    "\n",
    "            batch_state = paddle.to_tensor(batch_state,dtype=\"float32\")\n",
    "            batch_next_state = paddle.to_tensor(batch_next_state,dtype=\"float32\")\n",
    "            batch_action = paddle.to_tensor(batch_action,dtype=\"float32\").unsqueeze(1)\n",
    "            batch_reward = paddle.to_tensor(batch_reward,dtype=\"float32\").unsqueeze(1)\n",
    "\n",
    "\n",
    "            # 均方误差 y - Q(s, a) ， y是目标网络所看到的预期收益， 而 Q(s, a)是Critic网络预测的操作值。\n",
    "            # y是一个移动的目标，评论者模型试图实现的目标；这个目标通过缓慢的更新目标模型来保持稳定。\n",
    "            with paddle.no_grad():\n",
    "                Q_next = critic_target(batch_next_state, actor_target(batch_next_state))\n",
    "                Q_target = batch_reward + gamma * Q_next\n",
    "\n",
    "            critic_loss = F.mse_loss(critic(batch_state, batch_action), Q_target)\n",
    "\n",
    "\n",
    "            critic_optim.clear_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "            writer.add_scalar('critic loss', critic_loss.numpy(), learn_steps)\n",
    "            # 使用Critic网络给定值的平均值来评价Actor网络采取的行动。 我们力求使这一数值最大化。\n",
    "            # 因此，我们更新了Actor网络，对于一个给定状态，它产生的动作尽量让Critic网络给出高的评分。\n",
    "            critic.eval()\n",
    "            actor_loss = - critic(batch_state, actor(batch_state))\n",
    "            # print(actor_loss.shape)\n",
    "            actor_loss = actor_loss.mean()\n",
    "            actor_optim.clear_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "            critic.train()\n",
    "            writer.add_scalar('actor loss', actor_loss.numpy(), learn_steps)\n",
    "\n",
    "            soft_update(actor_target, actor, tau)\n",
    "            soft_update(critic_target, critic, tau)\n",
    "\n",
    "\n",
    "        if epsilon > 0:\n",
    "            epsilon -= 1 / explore\n",
    "        state = next_state\n",
    "\n",
    "    writer.add_scalar('episode reward', episode_reward, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:{}, episode reward is {}'.format(epoch, episode_reward))\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        paddle.save(actor.state_dict(), 'model/ddpg-actor' + str(epoch) + '.para')\n",
    "        paddle.save(critic.state_dict(), 'model/ddpg-critic' + str(epoch) + '.para')\n",
    "        print('model saved!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--env\", default=\"CircuitEnv\", help='CircuitEnv environment name')\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help='Sets Gym seed')\n",
    "    # parser.add_argument(\n",
    "    #     \"--train_total_steps\",\n",
    "    #     default=5e6,\n",
    "    #     type=int,\n",
    "    #     help='Max time steps to run environment')\n",
    "    # parser.add_argument(\n",
    "    #     '--test_every_steps',\n",
    "    #     type=int,\n",
    "    #     default=int(5e3),\n",
    "    #     help='The step interval between two consecutive evaluations')\n",
    "    args = parser.parse_args(args=[])\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'init_index'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [24]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43menviroment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCircuitEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m env\u001B[38;5;241m.\u001B[39mstate\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'init_index'"
     ]
    }
   ],
   "source": [
    "env = enviroment.CircuitEnv(local_index=0, init_index=12)\n",
    "env.state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actor = Actor()\n",
    "action = paddle.to_tensor(actor.select_action(epsilon=epsilon, state=env.state))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "critic = Critic()\n",
    "output = critic.forward(env.state, action)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}